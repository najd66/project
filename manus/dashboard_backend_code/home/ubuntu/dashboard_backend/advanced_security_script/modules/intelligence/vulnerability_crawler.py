# advanced_security_script/modules/intelligence/vulnerability_crawler.py

import logging
import asyncio
import aiohttp
import json
# from bs4 import BeautifulSoup # For parsing HTML if direct APIs are not available for all sources

logger = logging.getLogger(__name__)

class VulnerabilityCrawler:
    def __init__(self, config, data_manager):
        """
        Initializes the VulnerabilityCrawler.
        Args:
            config: Configuration object (from ConfigManager).
            data_manager: DataManager instance to potentially store crawled data or access API keys.
        """
        self.config = config
        self.data_manager = data_manager
        self.sources = {
            "nvd": self.config.get("vulnerability_crawler", "nvd_api_url", default="https://services.nvd.nist.gov/rest/json/cves/1.0"),
            # Add more sources like CVEmitre, Exploit-DB (might need web scraping or specific APIs)
            "exploit_db_rss": self.config.get("vulnerability_crawler", "exploit_db_rss", default="https://www.exploit-db.com/rss.xml"),
        }
        logger.info("VulnerabilityCrawler initialized.")

    async def fetch_from_source(self, session, source_name, url, params=None):
        """
        Fetches data from a single source URL.
        """
        try:
            logger.debug(f"Fetching data from {source_name} at {url} with params {params}")
            async with session.get(url, params=params, timeout=30) as response:
                response.raise_for_status() # Raise an exception for HTTP errors
                if "json" in response.content_type:
                    return await response.json()
                elif "xml" in response.content_type or "rss" in response.content_type:
                    # For XML/RSS, you might parse it differently (e.g., using xml.etree.ElementTree or feedparser)
                    return await response.text() # Placeholder, actual parsing needed
                else:
                    logger.warning(f"Unsupported content type {response.content_type} from {source_name}")
                    return None
        except aiohttp.ClientError as e:
            logger.error(f"Error fetching from {source_name} ({url}): {e}")
            return None
        except asyncio.TimeoutError:
            logger.error(f"Timeout fetching from {source_name} ({url})")
            return None

    async def crawl_vulnerabilities(self, keywords: list = None, max_results_per_source: int = 10) -> list:
        """
        Crawls various sources for the latest vulnerabilities.

        Args:
            keywords: Optional list of keywords to filter vulnerabilities (e.g., product names, technologies).
            max_results_per_source: Maximum number of results to fetch from each source (where applicable).

        Returns:
            A list of dictionaries, where each dictionary represents a found vulnerability.
        """
        logger.info(f"Starting vulnerability crawl. Keywords: {keywords}, Max results: {max_results_per_source}")
        all_vulnerabilities = []

        async with aiohttp.ClientSession() as session:
            tasks = []
            # NVD CVE API Example (fetches recent CVEs)
            if self.sources.get("nvd"):
                # NVD API has its own pagination and filtering, e.g., by keyword
                # For simplicity, fetching recent ones. A real implementation would handle pagination.
                nvd_params = {"resultsPerPage": max_results_per_source}
                if keywords:
                    nvd_params["keyword"] = " ".join(keywords) # NVD API keyword search
                tasks.append(self.fetch_from_source(session, "NVD", self.sources["nvd"], params=nvd_params))
            
            # Exploit-DB RSS feed example
            if self.sources.get("exploit_db_rss"):
                tasks.append(self.fetch_from_source(session, "Exploit-DB RSS", self.sources["exploit_db_rss"]))

            # Add more source fetching tasks here

            results = await asyncio.gather(*tasks)

            for i, result_data in enumerate(results):
                if result_data:
                    source_name = list(self.sources.keys())[i] # Assuming order is maintained
                    logger.info(f"Processing data from {source_name}")
                    if source_name == "NVD" and isinstance(result_data, dict) and "result" in result_data:
                        for cve_item in result_data.get("result", {}).get("CVE_Items", []):
                            cve_id = cve_item.get("cve", {}).get("CVE_data_meta", {}).get("ID")
                            description = "N/A"
                            if cve_item.get("cve", {}).get("description", {}).get("description_data"):
                                description = cve_item["cve"]["description"]["description_data"][0]["value"]
                            published_date = cve_item.get("publishedDate")
                            # Basic keyword filtering if API didn't do it or for refinement
                            if not keywords or any(kw.lower() in description.lower() or kw.lower() in cve_id.lower() for kw in keywords):
                                all_vulnerabilities.append({
                                    "source": "NVD",
                                    "id": cve_id,
                                    "description": description,
                                    "published_date": published_date,
                                    "link": f"https://nvd.nist.gov/vuln/detail/{cve_id}"
                                })
                    elif source_name == "Exploit-DB RSS" and isinstance(result_data, str):
                        # Placeholder: Actual RSS parsing needed here (e.g. using feedparser library)
                        # For now, just log that we got the data
                        logger.info(f"Received Exploit-DB RSS feed content. Length: {len(result_data)}. Parsing pending.")
                        # Example of what a parsed item might look like:
                        # all_vulnerabilities.append({
                        #     "source": "Exploit-DB",
                        #     "id": "EDB-ID:XXXXX", 
                        #     "title": "Exploit Title",
                        #     "published_date": "YYYY-MM-DD",
                        #     "link": "https://www.exploit-db.com/exploits/XXXXX"
                        # })
                    # Add processing for other sources here

        logger.info(f"Vulnerability crawl finished. Found {len(all_vulnerabilities)} potential vulnerabilities.")
        # The RAG/LLM part would take these raw_vulnerabilities and enrich/filter/summarize them.
        return all_vulnerabilities

if __name__ == '__main__':
    # Example Usage (for testing purposes)
    class MockConfig:
        def get(self, section, key, default=None):
            if key == "nvd_api_url": return "https://services.nvd.nist.gov/rest/json/cves/1.0"
            if key == "exploit_db_rss": return "https://www.exploit-db.com/rss.xml"
            return default
    class MockDataManager: pass

    logging.basicConfig(level=logging.INFO)
    crawler = VulnerabilityCrawler(MockConfig(), MockDataManager())
    
    async def main():
        # vulnerabilities = await crawler.crawl_vulnerabilities(keywords=["apache"], max_results_per_source=5)
        vulnerabilities = await crawler.crawl_vulnerabilities(max_results_per_source=5)
        print("\n--- Crawled Vulnerabilities ---")
        for vuln in vulnerabilities:
            print(json.dumps(vuln, indent=2))
        
        # Example with keyword
        # vulnerabilities_keyword = await crawler.crawl_vulnerabilities(keywords=["wordpress"], max_results_per_source=2)
        # print("\n--- Crawled Vulnerabilities (WordPress) ---")
        # for vuln in vulnerabilities_keyword:
        #     print(json.dumps(vuln, indent=2))
            
    asyncio.run(main())

